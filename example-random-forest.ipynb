{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d5fb36",
   "metadata": {},
   "source": [
    "# Random Forest Tutorial\n",
    "## A Complete Guide to Random Forest Classification and Regression\n",
    "\n",
    "This notebook covers:\n",
    "- Random Forest fundamentals\n",
    "- Classification example\n",
    "- Regression example\n",
    "- Feature importance\n",
    "- Hyperparameter tuning\n",
    "- Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97760c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afce9cd",
   "metadata": {},
   "source": [
    "## 1. Random Forest Classification Example\n",
    "### Using the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = iris.target\n",
    "\n",
    "print(\"Dataset shape:\", X_iris.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(X_iris.head())\n",
    "print(\"\\nTarget classes:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db45366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Create and train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_classifier.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_iris = rf_classifier.predict(X_test_iris)\n",
    "y_pred_proba_iris = rf_classifier.predict_proba(X_test_iris)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ab07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix - Random Forest Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47174a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_iris.columns,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance - Random Forest Classification')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f80cc3",
   "metadata": {},
   "source": [
    "## 2. Random Forest Regression Example\n",
    "### Using the California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfaf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore California housing dataset\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_california = pd.DataFrame(data=housing.data, columns=housing.feature_names)\n",
    "y_california = housing.target\n",
    "\n",
    "print(\"Dataset shape:\", X_california.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(X_california.head())\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(f\"Mean: {y_california.mean():.2f}\")\n",
    "print(f\"Std: {y_california.std():.2f}\")\n",
    "print(f\"Min: {y_california.min():.2f}\")\n",
    "print(f\"Max: {y_california.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49766b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_california, X_test_california, y_train_california, y_test_california = train_test_split(\n",
    "    X_california, y_california, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_regressor.fit(X_train_california, y_train_california)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_california = rf_regressor.predict(X_test_california)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test_california, y_pred_california)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_california, y_pred_california)\n",
    "\n",
    "print(\"Regression Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.3f}\")\n",
    "print(f\"Mean Absolute Error: {np.mean(np.abs(y_test_california - y_pred_california)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test_california, y_pred_california, alpha=0.6)\n",
    "plt.plot([y_test_california.min(), y_test_california.max()], \n",
    "         [y_test_california.min(), y_test_california.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test_california - y_pred_california\n",
    "plt.scatter(y_pred_california, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for regression\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': X_california.columns,\n",
    "    'importance': rf_regressor.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance_reg, x='importance', y='feature')\n",
    "plt.title('Feature Importance - Random Forest Regression')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(feature_importance_reg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae3f68",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40640ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid search for classification\n",
    "rf_grid = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    rf_grid, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Performing Grid Search...\")\n",
    "grid_search.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Test the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test_iris)\n",
    "print(f\"\\nTest Accuracy with Best Model: {best_rf.score(X_test_iris, y_test_iris):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea2f0f",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79de907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different n_estimators values\n",
    "n_estimators_range = [10, 50, 100, 200, 500]\n",
    "cv_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    rf_temp = RandomForestClassifier(n_estimators=n_est, random_state=42)\n",
    "    scores = cross_val_score(rf_temp, X_train_iris, y_train_iris, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "    print(f\"n_estimators={n_est}: CV Score = {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, cv_scores, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Random Forest Performance vs Number of Estimators')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d0a65",
   "metadata": {},
   "source": [
    "## 5. Understanding Random Forest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of max_features parameter\n",
    "max_features_options = ['sqrt', 'log2', None, 0.5]\n",
    "feature_results = []\n",
    "\n",
    "for max_feat in max_features_options:\n",
    "    rf_temp = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_features=max_feat, \n",
    "        random_state=42\n",
    "    )\n",
    "    scores = cross_val_score(rf_temp, X_train_iris, y_train_iris, cv=5)\n",
    "    feature_results.append({\n",
    "        'max_features': str(max_feat),\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std()\n",
    "    })\n",
    "\n",
    "feature_df = pd.DataFrame(feature_results)\n",
    "print(\"Effect of max_features parameter:\")\n",
    "print(feature_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_df['max_features'], feature_df['mean_score'], \n",
    "        yerr=feature_df['std_score'], capsize=5)\n",
    "plt.xlabel('max_features Parameter')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Effect of max_features on Random Forest Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9bbd8",
   "metadata": {},
   "source": [
    "## 6. Out-of-Bag (OOB) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f583b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with OOB scoring\n",
    "rf_oob = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_oob.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "print(f\"Out-of-Bag Score: {rf_oob.oob_score_:.3f}\")\n",
    "print(f\"Test Score: {rf_oob.score(X_test_iris, y_test_iris):.3f}\")\n",
    "\n",
    "# Compare OOB score with different n_estimators\n",
    "oob_scores = []\n",
    "estimator_range = range(10, 201, 10)\n",
    "\n",
    "for n_est in estimator_range:\n",
    "    rf_temp = RandomForestClassifier(\n",
    "        n_estimators=n_est, \n",
    "        oob_score=True, \n",
    "        random_state=42\n",
    "    )\n",
    "    rf_temp.fit(X_train_iris, y_train_iris)\n",
    "    oob_scores.append(rf_temp.oob_score_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(estimator_range, oob_scores, 'g-', linewidth=2)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('OOB Score')\n",
    "plt.title('Out-of-Bag Score vs Number of Estimators')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273cf73",
   "metadata": {},
   "source": [
    "## 7. Summary and Best Practices\n",
    "\n",
    "### Parameter Tuning Guidelines\n",
    "\n",
    "1. **Start with default parameters**, then tune if needed\n",
    "2. **n_estimators**: More trees = better performance (diminishing returns after ~100)\n",
    "3. **max_depth**: Control overfitting (None for full depth, 3-10 for regularization)\n",
    "4. **max_features**: `'sqrt'` for classification, `'log2'` or 1/3 for regression\n",
    "5. **min_samples_split/leaf**: Increase to prevent overfitting\n",
    "6. **Use OOB score** for quick model evaluation\n",
    "7. **Feature importance** helps with feature selection\n",
    "8. **Random Forest handles** missing values and mixed data types well\n",
    "9. **No need for feature scaling**\n",
    "10. **Use n_jobs=-1** for parallel processing\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- Reduces overfitting compared to single decision trees\n",
    "- Provides feature importance rankings\n",
    "- Handles both numerical and categorical features\n",
    "- Robust to outliers\n",
    "- Works well with default parameters\n",
    "\n",
    "### Key Disadvantages\n",
    "\n",
    "- Can overfit with very noisy data\n",
    "- Less interpretable than single decision trees\n",
    "- Memory intensive for large datasets\n",
    "- Biased towards categorical variables with more levels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
